{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4840219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a49c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    '''\n",
    "    A node object that is recursivly called within itslef to construct a regression tree. Based on Tianqi Chen's XGBoost \n",
    "    the internal gain used to find the optimal split value uses both the gradient and hessian. Also a weighted quantlie sketch \n",
    "    and optimal leaf values all follow Chen's description in \"XGBoost: A Scalable Tree Boosting System\" the only thing not \n",
    "    implemented in this version is sparsity aware fitting or the ability to handle NA values with a default direction.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, gradient, hessian, idxs, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "      \n",
    "        self.x, self.gradient, self.hessian = x, gradient, hessian\n",
    "        self.idxs = idxs \n",
    "        self.depth = depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.column_subsample = np.random.permutation(self.col_count)[:round(self.subsample_cols*self.col_count)]\n",
    "        \n",
    "        self.val = self.compute_gamma(self.gradient[self.idxs], self.hessian[self.idxs])\n",
    "          \n",
    "        self.score = float('-inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "        \n",
    "    def compute_gamma(self, gradient, hessian):\n",
    "        '''\n",
    "        Calculates the optimal leaf value equation (5) in \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        return(-np.sum(gradient)/(np.sum(hessian) + self.lambda_))\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        '''\n",
    "        Scans through every column and calcuates the best split point.\n",
    "        The node is then split at this point and two new nodes are created.\n",
    "        Depth is only parameter to change as we have added a new layer to tre structure.\n",
    "        If no split is better than the score initalised at the begining then no splits further splits are made\n",
    "        '''\n",
    "        for c in self.column_subsample: self.find_greedy_split(c)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[lhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        self.rhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[rhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        \n",
    "    def find_greedy_split(self, var_idx):\n",
    "        '''\n",
    "         For a given feature greedily calculates the gain at each split.\n",
    "         Globally updates the best score and split point if a better split point is found\n",
    "        '''\n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "        \n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= x[r])[0]\n",
    "            rhs_indices = np.nonzero(x > x[r])[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "\n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = x[r]\n",
    "                \n",
    "    def weighted_qauntile_sketch(self, var_idx):\n",
    "        '''\n",
    "        XGBOOST Mini-Version\n",
    "        Yiyang \"Joe\" Zeng\n",
    "        Is an approximation to the eact greedy approach faster for bigger datasets wher it is not feasible\n",
    "        to calculate the gain at every split point. Uses equation (8) and (9) from \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "        hessian_ = self.hessian[self.idxs]\n",
    "        df = pd.DataFrame({'feature':x,'hess':hessian_})\n",
    "        \n",
    "        df.sort_values(by=['feature'], ascending = True, inplace = True)\n",
    "        hess_sum = df['hess'].sum() \n",
    "        df['rank'] = df.apply(lambda x : (1/hess_sum)*sum(df[df['feature'] < x['feature']]['hess']), axis=1)\n",
    "        \n",
    "        for row in range(df.shape[0]-1):\n",
    "            # look at the current rank and the next ran\n",
    "            rk_sk_j, rk_sk_j_1 = df['rank'].iloc[row:row+2]\n",
    "            diff = abs(rk_sk_j - rk_sk_j_1)\n",
    "            if(diff >= self.eps):\n",
    "                continue\n",
    "                \n",
    "            split_value = (df['rank'].iloc[row+1] + df['rank'].iloc[row])/2\n",
    "            lhs = x <= split_value\n",
    "            rhs = x > split_value\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= split_value)[0]\n",
    "            rhs_indices = np.nonzero(x > split_value)[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "                \n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = split_value\n",
    "                \n",
    "    def gain(self, lhs, rhs):\n",
    "        '''\n",
    "        Calculates the gain at a particular split point bases on equation (7) from\n",
    "        \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        gradient = self.gradient[self.idxs]\n",
    "        hessian  = self.hessian[self.idxs]\n",
    "        \n",
    "        lhs_gradient = gradient[lhs].sum()\n",
    "        lhs_hessian  = hessian[lhs].sum()\n",
    "        \n",
    "        rhs_gradient = gradient[rhs].sum()\n",
    "        rhs_hessian  = hessian[rhs].sum()\n",
    "        \n",
    "        gain = 0.5 *( (lhs_gradient**2/(lhs_hessian + self.lambda_)) + (rhs_gradient**2/(rhs_hessian + self.lambda_)) - ((lhs_gradient + rhs_gradient)**2/(lhs_hessian + rhs_hessian + self.lambda_))) - self.gamma\n",
    "        return(gain)\n",
    "                \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        '''\n",
    "        splits a column \n",
    "        '''\n",
    "        return self.x.values[self.idxs , self.var_idx]\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        checks if node is a leaf\n",
    "        '''\n",
    "        return self.score == float('-inf') or self.depth <= 0                 \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "    \n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf:\n",
    "            return(self.val)\n",
    "\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ced1760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostTree:\n",
    "    '''\n",
    "    Wrapper class that provides a scikit learn interface to the recursive regression tree above\n",
    "    \n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    \n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    \n",
    "    '''\n",
    "    def fit(self, x, gradient, hessian, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "        self.dtree = Node(x, gradient, hessian, np.array(np.arange(len(x))), subsample_cols, min_leaf, min_child_weight, depth, lambda_, gamma, eps)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b21efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostClassifier:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    Binary Classification.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # first order gradient logLoss\n",
    "    def grad(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds - labels)\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    def hess(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds * (1 - preds))\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_odds(column):\n",
    "        binary_yes = np.count_nonzero(column == 1)\n",
    "        binary_no  = np.count_nonzero(column == 0)\n",
    "        return(np.log(binary_yes/binary_no))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "        self.X, self.y = X, y.values\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), 1).flatten().astype('float64')\n",
    "    \n",
    "        for booster in range(self.boosting_rounds):\n",
    "            Grad = self.grad(self.base_pred, self.y)\n",
    "            Hess = self.hess(self.base_pred, self.y)\n",
    "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "          \n",
    "    def predict_proba(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return(self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "        \n",
    "        predicted_probas = self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred)\n",
    "        preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)\n",
    "        return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1654103",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-01ab106268bc>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-01ab106268bc>\"\u001b[1;36m, line \u001b[1;32m64\u001b[0m\n\u001b[1;33m    ddef predict(self, X):\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class XGBoostRegressor:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    regression.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "    \n",
    "    # first order gradient mean squared error\n",
    "    @staticmethod\n",
    "    def grad(preds, labels):\n",
    "        return(2*(preds-labels))\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    @staticmethod\n",
    "    def hess(preds, labels):\n",
    "        '''\n",
    "        hessian of mean squared error is a constant value of two \n",
    "        returns an array of twos\n",
    "        '''\n",
    "        return(np.full((preds.shape[0], 1), 2).flatten().astype('float64'))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "        self.X, self.y = X, y.values\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), np.mean(y)).flatten().astype('float64')\n",
    "    \n",
    "        for booster in range(self.boosting_rounds):\n",
    "            Grad = self.grad(self.base_pred, self.y)\n",
    "            Hess = self.hess(self.base_pred, self.y)\n",
    "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return np.full((X.shape[0], 1), np.mean(y)).flatten().astype('float64') + pred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a381d9d3db2133197d2af93d93a0bc1bd0660cb0ffeb3da2d515607f4a9fcc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
